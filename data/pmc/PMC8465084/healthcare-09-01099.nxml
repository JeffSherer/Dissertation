<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Healthcare (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Healthcare (Basel)</journal-id><journal-id journal-id-type="publisher-id">healthcare</journal-id><journal-title-group><journal-title>Healthcare</journal-title></journal-title-group><issn pub-type="epub">2227-9032</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">34574873</article-id><article-id pub-id-type="pmc">8465084</article-id><article-id pub-id-type="doi">10.3390/healthcare9091099</article-id><article-id pub-id-type="publisher-id">healthcare-09-01099</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Detection of COVID-19 Patients from CT Scan and Chest X-ray Data Using Modified <italic toggle="yes">MobileNetV2</italic> and <italic toggle="yes">LIME</italic></article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0900-7930</contrib-id><name><surname>Ahsan</surname><given-names>Md Manjurul</given-names></name><xref rid="af1-healthcare-09-01099" ref-type="aff">1</xref><xref rid="c1-healthcare-09-01099" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Nazim</surname><given-names>Redwan</given-names></name><xref rid="af2-healthcare-09-01099" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2930-078X</contrib-id><name><surname>Siddique</surname><given-names>Zahed</given-names></name><xref rid="af3-healthcare-09-01099" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0027-463X</contrib-id><name><surname>Huebner</surname><given-names>Pedro</given-names></name><xref rid="af1-healthcare-09-01099" ref-type="aff">1</xref><xref rid="c1-healthcare-09-01099" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Saatchi</surname><given-names>Reza</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-healthcare-09-01099"><label>1</label>Industrial and Systems Engineering, University of Oklahoma, Norman, OK 73019, USA</aff><aff id="af2-healthcare-09-01099"><label>2</label>Chemical, Biological &#x00026; Materials Engineering, University of Oklahoma, Norman, OK 73019, USA; <email>redwan.nazim-1@ou.edu</email></aff><aff id="af3-healthcare-09-01099"><label>3</label>School of Aerospace and Mechanical Engineering, University of Oklahoma, Norman, OK 73019, USA; <email>zsiddique@ou.edu</email></aff><author-notes><corresp id="c1-healthcare-09-01099"><label>*</label>Correspondence: <email>ahsan@ou.edu</email> (M.M.A.); <email>pedro@ou.edu</email> (P.H.)</corresp></author-notes><pub-date pub-type="epub"><day>25</day><month>8</month><year>2021</year></pub-date><pub-date pub-type="collection"><month>9</month><year>2021</year></pub-date><volume>9</volume><issue>9</issue><elocation-id>1099</elocation-id><history><date date-type="received"><day>16</day><month>6</month><year>2021</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2021</year></date></history><permissions><copyright-statement>&#x000a9; 2021 by the authors.</copyright-statement><copyright-year>2021</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>The COVID-19 global pandemic caused by the widespread transmission of the novel coronavirus (SARS-CoV-2) has become one of modern history&#x02019;s most challenging issues from a healthcare perspective. At its dawn, still without a vaccine, contagion containment strategies remained most effective in preventing the disease&#x02019;s spread. Patient isolation has been primarily driven by the results of polymerase chain reaction (PCR) testing, but its initial reach was challenged by low availability and high cost, especially in developing countries. As a means of taking advantage of a preexisting infrastructure for respiratory disease diagnosis, researchers have proposed COVID-19 patient screening based on the results of Chest Computerized Tomography (CT) and Chest Radiographs (X-ray). When paired with artificial-intelligence- and deep-learning-based approaches for analysis, early studies have achieved a comparatively high accuracy in diagnosing the disease. Considering the opportunity to further explore these methods, we implement six different Deep Convolutional Neural Networks (Deep CNN) models&#x02014;VGG16, MobileNetV2, InceptionResNetV2, ResNet50, ResNet101, and VGG19&#x02014;and use a mixed dataset of CT and X-ray images to classify COVID-19 patients. Preliminary results showed that a modified MobileNetV2 model performs best with an accuracy of 95 &#x000b1; 1.12% (AUC = 0.816). Notably, a high performance was also observed for the VGG16 model, outperforming several previously proposed models with an accuracy of 98.5 &#x000b1; 1.19% on the X-ray dataset. Our findings are supported by recent works in the academic literature, which also uphold the higher performance of MobileNetV2 when X-ray, CT, and their mixed datasets are considered. Lastly, we further explain the process of feature extraction using Local Interpretable Model-Agnostic Explanations (LIME), which contributes to a better understanding of what features in CT/X-ray images characterize the onset of COVID-19.</p></abstract><kwd-group><kwd>chest X-ray</kwd><kwd>CT scan</kwd><kwd>coronavirus</kwd><kwd>COVID-19</kwd><kwd>deep learning</kwd><kwd>imbalanced data</kwd><kwd>mixed-data</kwd><kwd>SARS-CoV-2</kwd><kwd>small data</kwd><kwd>explainable AI</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-healthcare-09-01099"><title>1. Introduction</title><p>The novel coronavirus (SARS-CoV-2) global pandemic has represented one of humanity&#x02019;s greatest challenges in modern history. For most of the now year-and-a-half long crisis, a vaccine, despite having accelerated development due to the global emergency, remained unavailable for most people. The advent of the new COVID-19 delta strain introduced another layer of concern as rates of transmission and resistance to select vaccines are notably high. According to recent guidelines from the US Center for Disease Control and Prevention (CDC), vaccinated individuals should continue to wear masks to prevent viral transmission and the infection of unvaccinated individuals [<xref rid="B1-healthcare-09-01099" ref-type="bibr">1</xref>]. Statistically, the number of affected individuals and casualties are astounding and alarming: 200,237,344 and 4,258,459, respectively, as of 3 August 2021 [<xref rid="B2-healthcare-09-01099" ref-type="bibr">2</xref>], with an associated mortality rate of about 2.13 percent. As a measure to reduce the spread of the virus&#x02014;which transmits itself through close contact and respiratory droplets of infected individuals while talking, coughing, or sneezing&#x02014;many countries prohibited any social gathering in community, work, and school, and forced citizens into mandatory lockdowns and quarantining. A key opportunity to minimize the spread is to correctly diagnose infected individuals; currently, real-time reverse transcription-polymerase chain reaction (RT-PCR) is used as a gold-standard test to diagnose the onset of COVID-19 [<xref rid="B3-healthcare-09-01099" ref-type="bibr">3</xref>,<xref rid="B4-healthcare-09-01099" ref-type="bibr">4</xref>]. However, the limitations surrounding the depth of our understanding regarding the nature of the virus, testing kits may be associated with a high error rate, approaching 30% [<xref rid="B2-healthcare-09-01099" ref-type="bibr">2</xref>]. Inaccurate testing has been credited as one of the many contributing factors of ineffective disease containment. As a result, researchers have proposed alternative approaches, such as chest X-ray- and CT-scan-based patient diagnosis as options to support the early identification of individuals potentially carrying the virus. Such techniques can take great advantage of current deep-learning- and artificial-intelligence (AI)-based methods applied to either small data [<xref rid="B5-healthcare-09-01099" ref-type="bibr">5</xref>,<xref rid="B6-healthcare-09-01099" ref-type="bibr">6</xref>,<xref rid="B7-healthcare-09-01099" ref-type="bibr">7</xref>,<xref rid="B8-healthcare-09-01099" ref-type="bibr">8</xref>] or large datasets [<xref rid="B5-healthcare-09-01099" ref-type="bibr">5</xref>,<xref rid="B9-healthcare-09-01099" ref-type="bibr">9</xref>,<xref rid="B10-healthcare-09-01099" ref-type="bibr">10</xref>]. For instance, Chen et al. (2020) proposed a UNet++ model using a small dataset containing 51 COVID-19 and 82 non-COVID-19 patients and achieved an accuracy of around 98.5% [<xref rid="B6-healthcare-09-01099" ref-type="bibr">6</xref>]. Similarly, Ardakani et al. (2020), used a small dataset of 108 COVID-19 and 86 non-COVID-19 patients to test ten different deep learning models and obtained a 99% accuracy overall [<xref rid="B7-healthcare-09-01099" ref-type="bibr">7</xref>]. Wang et al. (2020) proposed an inception-based model utilizing a comparatively large dataset, with 453 CT scan images being incorporated in the analysis, ultimately obtaining an accuracy of 73.1% [<xref rid="B9-healthcare-09-01099" ref-type="bibr">9</xref>]. However, along with lower accuracy, the model&#x02019;s network activity and region of interest were not clearly explained. Lastly, Li et al. (2020) used a moderately large dataset containing 4356 chest CT images of pneumonia patients, of which 1296 were confirmed COVID-19 cases, and obtained 96% accuracy with the proposed COVNet model [<xref rid="B5-healthcare-09-01099" ref-type="bibr">5</xref>].</p><p>In parallel, several studies explored and recommended screening COVID-19 patients using chest X-ray images instead&#x02014;notable contributions can be found in [<xref rid="B11-healthcare-09-01099" ref-type="bibr">11</xref>,<xref rid="B12-healthcare-09-01099" ref-type="bibr">12</xref>,<xref rid="B13-healthcare-09-01099" ref-type="bibr">13</xref>]. For instance, Hemdan et al. (2020) worked on a small dataset, comprising only 50 images, and demonstrated an accuracy of 90% and 95% in predicting COVID-19 patients from chest X-ray images using VGG19 and ResNet50 models, respectively [<xref rid="B11-healthcare-09-01099" ref-type="bibr">11</xref>]. Using a dataset of 100 images, Narin et al. (2020) distinguished COVID-19 patients from those with pneumonia with 86% accuracy [<xref rid="B13-healthcare-09-01099" ref-type="bibr">13</xref>]. However, due to the relatively small dataset, questions remain regarding the model&#x02019;s stability and interpretability. To address these issues, our previous work has focused on representing the performance of different deep learning models with 95% confidence intervals, so as to understand and better interpret their performance on small datasets. For example, with a data pool of 50 chest X-ray images, we found that InceptionResNetV2 models identify COVID-19 patients with 97% accuracy, but with the Wilson score method representing an accuracy in the range of 68.1% to 99.8%. Besides, the study also revealed that deep CNN-based architecture, such as VGG16 and ResNet50, often extract unnecessary features from the images, especially when applied on very small datasets. For instance, a modified VGG16 model identified 97% of COVID-19 patients correctly, but the model architecture emphasized a significant amount of features in the region of the collarbone and upper shoulder instead of the region of interest on the chest and lungs, as shown in <xref rid="healthcare-09-01099-f001" ref-type="fig">Figure 1</xref>.</p><p>However, a significant improvement was observed utilizing a comparatively larger dataset of 1845 chest X-ray images, which ultimately demonstrated higher accuracy [<xref rid="B14-healthcare-09-01099" ref-type="bibr">14</xref>]. Models trained with such big data convey the advantages over small data by reducing unnecessary or irrelevant feature detection on chest X-ray images, as shown in <xref rid="healthcare-09-01099-f002" ref-type="fig">Figure 2</xref>.</p><p>Researchers often train their models with large chest X-ray image datasets [<xref rid="B15-healthcare-09-01099" ref-type="bibr">15</xref>,<xref rid="B16-healthcare-09-01099" ref-type="bibr">16</xref>] in order to develop a robust model. For example, 6505 images with a data ratio of 1:1.17 were utilized by Brunese et al. (2020), wherein 3003 images were patients with COVID-19 symptoms, and 3520 were labeled as &#x0201c;other patients&#x0201d; for the purposes of that study [<xref rid="B15-healthcare-09-01099" ref-type="bibr">15</xref>]. Ghoshal and Tucker (2020) used a dataset of 5941 images and achieved 92.9% accuracy [<xref rid="B16-healthcare-09-01099" ref-type="bibr">16</xref>]. However, neither study assessed or discussed how their proposed models would perform with highly imbalanced data containing unequal class ratio. On that note, Apostol, Oztuk, and Khan (2020) considered an imbalanced dataset of 284 COVID-19 and 967 non-COVID-19 patient chest X-ray images and achieved 89.6% accuracy using a CNN-based Xception model [<xref rid="B17-healthcare-09-01099" ref-type="bibr">17</xref>]. Despite the demonstrated potential, challenges associated with the unequal data ratio, such as the risk of overfitting or underfitting during the training stages, were not explored in detail. Considering those opportunities and the rapid spread of a transmittable disease such as COVID-19, we recognize that existing resources and methodologies are not alone sufficient to serve as a reliable means of diagnosis during the early stages of a rapidly spreading pandemic. Thus, instead of using only chest CT or X-ray-based screening, a better solution lies in integrating the usage of both techniques. A few advantages of this proposed method include more patients being able to get tested, and less reliability on COVID 19 testing kits. We explore this opportunity and investigate a reliable and explainable AI-based COVID-19 screening system that can identify symptomatic patients from widely available medical image data. In this study, we apply and evaluate the performance of several AI-based models with a mixed dataset containing both chest CT and X-ray images. We summarize our main contributions as follows:<list list-type="bullet"><list-item><p>Implementation and evaluation of six different deep CNN models (VGG16 [<xref rid="B18-healthcare-09-01099" ref-type="bibr">18</xref>], InceptionResNetV2 [<xref rid="B19-healthcare-09-01099" ref-type="bibr">19</xref>], ResNet50 [<xref rid="B20-healthcare-09-01099" ref-type="bibr">20</xref>], MobileNetV2 [<xref rid="B21-healthcare-09-01099" ref-type="bibr">21</xref>], ResNet101 [<xref rid="B22-healthcare-09-01099" ref-type="bibr">22</xref>], and VGG19 [<xref rid="B18-healthcare-09-01099" ref-type="bibr">18</xref>]) to detect COVID-19 patients using a mixed dataset of chest CT and X-ray images;</p></list-item><list-item><p>A detailed analysis of the results obtained and comparison with the performance of the same models being applied to independent datasets of either CT scans or X-ray images;</p></list-item><list-item><p>Finally, we explain the models&#x02019; predictability considering top features with Local Interpretable Model-Agnostic Explanations (LIME).</p></list-item></list></p></sec><sec sec-type="methods" id="sec2-healthcare-09-01099"><title>2. Research Methodology</title><p><xref rid="healthcare-09-01099-t001" ref-type="table">Table 1</xref> summarizes our adopted dataset [<xref rid="B23-healthcare-09-01099" ref-type="bibr">23</xref>], which contains both CT scans (200 COVID-19 and 200 Non-COVID-19) and chest X-rays (1583 COVID-19 and 608 Non-COVID-19) of patients expressing pneumonia symptoms. We dedicated 80% of the data for training and the remaining 20% for testing. <xref rid="healthcare-09-01099-f003" ref-type="fig">Figure 3</xref> presents a set of representative images used in the analysis.</p><sec id="sec2dot1-healthcare-09-01099"><title>2.1. Using Pre-Trained Convolutional Networks</title><p>We used six different pre-trained ConvNets: VGG16, MobileNetV2, ResNet50, ResNet101, InceptionResNetV2, and VGG19. A comprehensive explanation of the network&#x02019;s architecture can be found in [<xref rid="B24-healthcare-09-01099" ref-type="bibr">24</xref>]. Each model is developed with the advantages of transfer learning. The modified architecture was developed using the following steps:<list list-type="order"><list-item><p>Models are initiated with the pre-trained network without a fully connected (FC) layer;</p></list-item><list-item><p>A new layer is added, containing &#x0201c;Maxpool&#x0201d; and &#x0201c;softmax&#x0201d; as activation functions and appended with the network&#x02019;s primary architecture;</p></list-item><list-item><p>The convolutional weights are kept frozen and only the new FC layers are trained with the new CNN architecture.</p></list-item></list></p><p>A similar procedure was applied for the other five proposed deep CNN models. The constructed CNN architecture has the following sequence: AveragePooling2D (Poolsize= (4.4))&#x02014;Flatten&#x02014;Dense&#x02014;Dropout (0.5)&#x02014;Dense (Activation = &#x0201c;Softmax&#x0201d;). Three parameters, specifically batch size, epochs, and learning rate (as suggested by [<xref rid="B25-healthcare-09-01099" ref-type="bibr">25</xref>,<xref rid="B26-healthcare-09-01099" ref-type="bibr">26</xref>]), are considered for model optimization. We adopted the commonly employed grid search method [<xref rid="B27-healthcare-09-01099" ref-type="bibr">27</xref>] to fine tune parameters. At first, the following were chosen at random:<disp-formula>Batch size = [20, 30, 40, 50, 60];</disp-formula><disp-formula>Number of epochs = [20, 25, 30, 35, 40];</disp-formula><disp-formula>Learning rate = [0.001, 0.01, 0.1].</disp-formula></p><p>Following the final computation, best results were obtained with the following:<disp-formula>Batch size = 50;</disp-formula><disp-formula>Number of epochs = 35;</disp-formula><disp-formula>Learning rate = 0.001.</disp-formula></p><p>Adaptive learning rate optimization, also known as Adam [<xref rid="B28-healthcare-09-01099" ref-type="bibr">28</xref>,<xref rid="B29-healthcare-09-01099" ref-type="bibr">29</xref>], was used as an optimization algorithm as used in previous works [<xref rid="B14-healthcare-09-01099" ref-type="bibr">14</xref>]. The experimental procedure was run twice, and the results were obtained by averaging the two results. The statistical analysis was evaluated in terms of accuracy, precision, recall, and f-1 score [<xref rid="B30-healthcare-09-01099" ref-type="bibr">30</xref>], as defined below:<disp-formula id="FD1-healthcare-09-01099"><label>(1)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2-healthcare-09-01099"><label>(2)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3-healthcare-09-01099"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4-healthcare-09-01099"><label>(4)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where,</p><p>True positive (<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm5" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) = COVID-19 infectious patients classified as patients;</p><p>False Positive (<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) = Healthy people classified as COVID-19 patients;</p><p>True Negative (<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) = Healthy people classified as healthy;</p><p>False Negative (<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) = COVID-19 infectious patients classified as healthy.</p></sec><sec id="sec2dot2-healthcare-09-01099"><title>2.2. LIME as Explainable AI</title><p>The overall prediction was interpreted using LIME, a procedure that allows the understanding of the input features of the deep learning models which affect its predictions. LIME is regarded as one of the few methodologies that works well with tabular data, text, and images, and is extensively employed for its reliability in explaining the intricacies of image classification [<xref rid="B31-healthcare-09-01099" ref-type="bibr">31</xref>]. For image classification, LIME creates superpixels. Superpixels are the result of image over-segmentation. Superpixels store more data than pixels and are more aligned with image edges than rectangular image patches [<xref rid="B32-healthcare-09-01099" ref-type="bibr">32</xref>]) for the primary prediction. <xref rid="healthcare-09-01099-t002" ref-type="table">Table 2</xref> shows the parameters used to calculate the superpixel during this experiment.</p></sec></sec><sec sec-type="results" id="sec3-healthcare-09-01099"><title>3. Results</title><p><xref rid="healthcare-09-01099-t003" ref-type="table">Table 3</xref> presents a summary of the performance of all models on the training and test sets along with a 95% confidence interval. MobileNetV2 outperformed all models in terms of accuracy, precision, recall, and f-1 score. Contrarily, the ResNet50 model showed the worst performance considering all measures.</p><p>To better understand the overall performance of each model during the prediction stage on the test set, <xref rid="healthcare-09-01099-f004" ref-type="fig">Figure 4</xref> presents a set of confusion matrices. The test set contained a combination of 519 chest X-ray and CT scan images (122 COVID-19 and 397 Non-COVID-19). It can be detected that MobileNetV2 and VGG19 correctly classified the maximum number of COVID-19 and non-COVID-19 patients, whereas ResNet50 expressed the worst performance with the maximum number of misclassified samples compared to any other model.</p><p>The performance of all models during training and testing, per each epoch, are presented in <xref rid="healthcare-09-01099-f005" ref-type="fig">Figure 5</xref>. In this case, the accuracy of VGG16, MobileNetV2, and VGG19 models reached 100% while loss decreased by nearly 100% at epoch 35.</p><sec><title>AUC-ROC Curve</title><p>In <xref rid="healthcare-09-01099-f006" ref-type="fig">Figure 6</xref>, measures of the Area Under the Curve (AUC) of the Receiver Characteristic Operator (ROC) are plotted for each model with the true positive rate (TPR) in the vertical axis and false positive rate (FPR) in the horizontal axis, applied to the test set. MobileNetV2 shows the best performance (AUC=0.816), while ResNet101 shows the worst (AUC=0.590).</p><p><xref rid="healthcare-09-01099-f007" ref-type="fig">Figure 7</xref> shows the output after computing the superpixels on sample CT scan and chest X-ray images.</p><p>Additionally, <xref rid="healthcare-09-01099-f008" ref-type="fig">Figure 8</xref> shows different image conditions in terms of perturbation vectors and perturbation images. <xref rid="healthcare-09-01099-f008" ref-type="fig">Figure 8</xref> illustrates that the number of features varies with the number of perturbations.</p><p>The distance metric or cosine metric with a kernel width of 0.25 is used to understand the distance difference between each perturbation and the original image. A linear model is used for the proposed model&#x02019;s explanations. Additionally, the coefficient was found for every superpixel in the picture which represents the strength of a superpixel&#x02019;s impact on predicting COVID-19 patients. Finally, top features (only four features are considered for the purposes of this study) are sorted to determine the most essential superpixel, as shown in <xref rid="healthcare-09-01099-f009" ref-type="fig">Figure 9</xref>. The features and the prediction were addressed together during this study. As shown in <xref rid="healthcare-09-01099-f009" ref-type="fig">Figure 9</xref>, models, such as VGG16, MobileNetV2 and VGG19 trained with CT scan images incorrectly classified COVID-19 patients as Non-COVID-19 patients. On the other hand, while analyzing combined models, ResNet50 shows the worst performance by misclassifying both CT and chest X-ray images.</p></sec></sec><sec sec-type="discussion" id="sec4-healthcare-09-01099"><title>4. Discussion</title><p>In this study, six different deep-learning-based models were proposed and evaluated for their ability to distinguish between patients with and without COVID-19, with demonstrated advantages of tests conducted on combined datasets, comprising both CT scan and X-ray images (as opposed to a singular point of reference with only CT scans or X-rays). Among all proposed models, MobileNetV2 achieved an accuracy of 95 &#x000b1; 1.12% depending on the dataset applied. A summary of the accuracy of all six models, considering the CT scan, chest X-ray, and the mixed dataset is presented in <xref rid="healthcare-09-01099-t004" ref-type="table">Table 4</xref>. Other than MobileNetV2, the VGG16 model demonstrates higher performance on X-ray dataset by achieving an accuracy of 98.5% &#x000b1; 1.19%, which outperforms many studies in the current literature. For example, Wang and Wong (2020) [<xref rid="B9-healthcare-09-01099" ref-type="bibr">9</xref>] and Khan et al. (2020) [<xref rid="B33-healthcare-09-01099" ref-type="bibr">33</xref>] used CNN-based approaches to detect the onset of the COVID-19 disease using chest X-ray images and achieved an accuracy of 83.5% and 89.6%, respectively. In comparison, as previously stated, our proposed VGG16 and MobileNetV2 models achieved an accuracy of around 98.5% &#x000b1; 1.19%.</p><p>In <xref rid="healthcare-09-01099-t005" ref-type="table">Table 5</xref>, the accuracy of different deep learning models used in previous studies are compared (where CT scan images were used for the experiment) with the models of this study in consideration of different database sizes. Here, an accuracy of 98.5% &#x000b1; 1.19% was achieved using 400 images with the MobileNetV2 model. These results outperform the referenced literature [<xref rid="B34-healthcare-09-01099" ref-type="bibr">34</xref>,<xref rid="B35-healthcare-09-01099" ref-type="bibr">35</xref>], which used large datasets containing 4356 and 1065 images, respectively. In contrast, Butt et al. (2020) used a CNN-based approach, specifically a ResNet23 model to detect the onset of COVID-19 disease using chest CT scan images and achieved an accuracy of around 86.7% [<xref rid="B10-healthcare-09-01099" ref-type="bibr">10</xref>]. Jin et al. (2020) used 1882 CT scan images and achieved an accuracy of 94.1% [<xref rid="B36-healthcare-09-01099" ref-type="bibr">36</xref>].</p><p>It is relevant to emphasize that none of the referenced literature considered a mixed-dataset, which hinders a direct comparison with the results of this study. However, preliminary computational results on a mixed dataset indicated that a modified MobileNetV2 model is capable of differentiating between patients with COVID-19 symptoms with an accuracy of 95% &#x000b1; 1.12%. Additionally, analyzing the proposed models with LIME illustrated MobileNetV2&#x02019;s contribution to successfully characterizing the onset of COVID-19 by recognizing essential features in CT/X-ray images.</p><p>The primary goal of this study was to develop an integrated system that can detect patients with COVID-19 symptoms from a dataset containing CT scan, chest X-ray, or a combination of CT scan and chest X-ray images of potential COVID-19 patients. At this stage, the scope of the current literature in this field of work remains narrow and often does not consider combined CT scan and chest X-ray image datasets with explainable AI. Here, predicted features were identified with LIME to understand the models&#x02019; decision-making process. Going forward, results of studies such as the one herein presented must be verified in consultation with healthcare experts. In addition, future work can take advantage of evaluating how other interpretable models could be used with mixed datasets in an attempt to validate the overall predictions presented here.</p></sec><sec sec-type="conclusions" id="sec5-healthcare-09-01099"><title>5. Conclusions</title><p>In this study, we evaluated six different deep learning models on a mixed dataset of CT scan and chest X-ray images for their ability to identify COVID-19 patients. We revealed that a modified MobileNetV2 can achieve an accuracy of 95% on that task. We have also used Local Interpretable Model-Agnostic Explanations (LIME) to interpret and validate our predictions. The findings of the proposed models should provide some insights to researchers and practitioners regarding the application of explainable AI on screening COVID-19 patients based on chest X-ray and CT-scan images. Next steps which would build on the efforts of our work include developing user-friendly mobile apps/web-based COVID-19 screening systems using MobileNetV2 models and creating decision support systems along with numerical (i.e., age, gender) and categorical (findings, health conditions) data. Opportunities also lie in utilizing other image processing techniques, such as fuzzy entropy and divergence, so as to more precisely recognize edges and contours of X-rays and CT images [<xref rid="B39-healthcare-09-01099" ref-type="bibr">39</xref>,<xref rid="B40-healthcare-09-01099" ref-type="bibr">40</xref>]. </p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.M.A. and R.N.; methodology, M.M.A., P.H.; software, M.M.A.; validation, P.H., Z.S. and R.N.; formal analysis, P.H. and Z.S.; investigation, R.N., Z.S.; writing&#x02014;original draft preparation, M.M.A. and R.N.; writing&#x02014;review and editing, P.H., R.N. and Z.S.; visualization, M.M.A.; supervision, P.H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Funding</title><p>This research received no external funding.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Not applicable.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-healthcare-09-01099"><label>1.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Guardian</surname><given-names>T.</given-names></name>
</person-group><article-title>New US Mask Guidance Prompted by Evidence Vaccinated Can Spread Delta. August 2021</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.theguardian.com/world/2021/jul/28/cdc-director-new-mask-guidance-vaccinated-spreading-delta-variant" ext-link-type="uri">https://www.theguardian.com/world/2021/jul/28/cdc-director-new-mask-guidance-vaccinated-spreading-delta-variant</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2021-08-03">(accessed on 3 August 2021)</date-in-citation></element-citation></ref><ref id="B2-healthcare-09-01099"><label>2.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>Dashbord</collab>
</person-group><article-title>Covid-19 WorldMeter. June 2020</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.worldometers.info/coronavirus/" ext-link-type="uri">https://www.worldometers.info/coronavirus/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2021-08-03">(accessed on 3 August 2021)</date-in-citation></element-citation></ref><ref id="B3-healthcare-09-01099"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Haghanifar</surname><given-names>A.</given-names></name>
<name><surname>Majdabadi</surname><given-names>M.M.</given-names></name>
<name><surname>Ko</surname><given-names>S.</given-names></name>
</person-group><article-title>COVID-CXNet: Detecting COVID-19 in Frontal Chest X-ray Images using Deep Learning</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2006.13807</pub-id></element-citation></ref><ref id="B4-healthcare-09-01099"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tahamtan</surname><given-names>A.</given-names></name>
<name><surname>Ardebili</surname><given-names>A.</given-names></name>
</person-group><article-title>Real-time RT-PCR in COVID-19 detection: Issues affecting the results</article-title><source>Expert Rev. Mol. Diagn.</source><year>2020</year><volume>20</volume><fpage>453</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1080/14737159.2020.1757437</pub-id><pub-id pub-id-type="pmid">32297805</pub-id></element-citation></ref><ref id="B5-healthcare-09-01099"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>T.</given-names></name>
</person-group><article-title>Handbook of COVID-19 Prevention and Treatment</article-title><source>First Affil. Hosp. Zhejiang Univ. Sch. Med. Compil. Accord. Clin. Exp.</source><year>2020</year><volume>68</volume><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.alnap.org/help-library/handbook-of-covid-19-prevention-and-treatment" ext-link-type="uri">https://www.alnap.org/help-library/handbook-of-covid-19-prevention-and-treatment</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2021-02-15">(accessed on 15 February 2021)</date-in-citation></element-citation></ref><ref id="B6-healthcare-09-01099"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Gong</surname><given-names>D.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
<name><surname>Zheng</surname><given-names>B.</given-names></name>
<etal/>
</person-group><article-title>Deep Learning-Based Model for Detecting 2019 Novel Coronavirus Pneumonia on High-Resolution Computed Tomography: A Prospective Study</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">31913322</pub-id></element-citation></ref><ref id="B7-healthcare-09-01099"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ardakani</surname><given-names>A.A.</given-names></name>
<name><surname>Kanafi</surname><given-names>A.R.</given-names></name>
<name><surname>Acharya</surname><given-names>U.R.</given-names></name>
<name><surname>Khadem</surname><given-names>N.</given-names></name>
<name><surname>Mohammadi</surname><given-names>A.</given-names></name>
</person-group><article-title>Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: Results of 10 convolutional neural networks</article-title><source>Comput. Biol. Med.</source><year>2020</year><volume>121</volume><fpage>103795</fpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2020.103795</pub-id><pub-id pub-id-type="pmid">32568676</pub-id></element-citation></ref><ref id="B8-healthcare-09-01099"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>C.</given-names></name>
<name><surname>Deng</surname><given-names>X.</given-names></name>
<name><surname>Fu</surname><given-names>Q.</given-names></name>
<name><surname>Zhou</surname><given-names>Q.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
<name><surname>Ma</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
</person-group><article-title>Deep learning-based detection for COVID-19 from chest CT using weak label</article-title><source>MedRxiv</source><year>2020</year><pub-id pub-id-type="doi">10.1101/2020.03.12.20027185</pub-id></element-citation></ref><ref id="B9-healthcare-09-01099"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Wong</surname><given-names>A.</given-names></name>
</person-group><article-title>Covid-net: A Tailored Deep Convolutional Neural Network Design for Detection of Covid-19 Cases from Chest X-ray Images</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.09871</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-76550-z</pub-id></element-citation></ref><ref id="B10-healthcare-09-01099"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Shi</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Zhao</surname><given-names>P.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Zhu</surname><given-names>L.</given-names></name>
<etal/>
</person-group><article-title>Pathological Findings of COVID-19 Associated with Acute Respiratory Distress Syndrome</article-title><source>Lancet Respir. Med.</source><year>2020</year><volume>8</volume><fpage>420</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1016/S2213-2600(20)30076-X</pub-id><pub-id pub-id-type="pmid">32085846</pub-id></element-citation></ref><ref id="B11-healthcare-09-01099"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hemdan</surname><given-names>E.E.D.</given-names></name>
<name><surname>Shouman</surname><given-names>M.A.</given-names></name>
<name><surname>Karar</surname><given-names>M.E.</given-names></name>
</person-group><article-title>Covidx-Net: A Framework of Deep Learning Classifiers to Diagnose Covid-19 in X-ray Images</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.11055</pub-id></element-citation></ref><ref id="B12-healthcare-09-01099"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sethy</surname><given-names>P.K.</given-names></name>
<name><surname>Behera</surname><given-names>S.K.</given-names></name>
</person-group><article-title>Detection of Coronavirus Disease (Covid-19) Based on Deep Features</article-title><source>Preprints</source><year>2020</year><pub-id pub-id-type="doi">10.20944/preprints202003.0300.v1</pub-id></element-citation></ref><ref id="B13-healthcare-09-01099"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Narin</surname><given-names>A.</given-names></name>
<name><surname>Kaya</surname><given-names>C.</given-names></name>
<name><surname>Pamuk</surname><given-names>Z.</given-names></name>
</person-group><article-title>Automatic Detection of Coronavirus Disease (covid-19) Using X-ray Images and Deep Convolutional Neural Networks</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.10849</pub-id></element-citation></ref><ref id="B14-healthcare-09-01099"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ahsan</surname><given-names>M.M.</given-names></name>
<name><surname>Ahad</surname><given-names>M.T.</given-names></name>
<name><surname>Soma</surname><given-names>F.A.</given-names></name>
<name><surname>Paul</surname><given-names>S.</given-names></name>
<name><surname>Chowdhury</surname><given-names>A.</given-names></name>
<name><surname>Luna</surname><given-names>S.A.</given-names></name>
<name><surname>Yazdan</surname><given-names>M.M.S.</given-names></name>
<name><surname>Rahman</surname><given-names>A.</given-names></name>
<name><surname>Siddique</surname><given-names>Z.</given-names></name>
<name><surname>Huebner</surname><given-names>P.</given-names></name>
</person-group><article-title>Detecting SARS-CoV-2 from Chest X-ray using Artificial Intelligence</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>35501</fpage><lpage>35513</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3061621</pub-id></element-citation></ref><ref id="B15-healthcare-09-01099"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brunese</surname><given-names>L.</given-names></name>
<name><surname>Mercaldo</surname><given-names>F.</given-names></name>
<name><surname>Reginelli</surname><given-names>A.</given-names></name>
<name><surname>Santone</surname><given-names>A.</given-names></name>
</person-group><article-title>Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays</article-title><source>Comput. Methods Programs Biomed.</source><year>2020</year><volume>196</volume><fpage>105608</fpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105608</pub-id><pub-id pub-id-type="pmid">32599338</pub-id></element-citation></ref><ref id="B16-healthcare-09-01099"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghoshal</surname><given-names>B.</given-names></name>
<name><surname>Tucker</surname><given-names>A.</given-names></name>
</person-group><article-title>Estimating Uncertainty and Interpretability in Deep Learning for Coronavirus (COVID-19) Detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.10769</pub-id></element-citation></ref><ref id="B17-healthcare-09-01099"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Apostolopoulos</surname><given-names>I.D.</given-names></name>
<name><surname>Mpesiana</surname><given-names>T.A.</given-names></name>
</person-group><article-title>Covid-19: Automatic Detection from X-ray Images Utilizing Transfer Learning with Convolutional Neural Networks</article-title><source>Phys. Eng. Sci. Med.</source><year>2020</year><volume>43</volume><fpage>635</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1007/s13246-020-00865-4</pub-id><pub-id pub-id-type="pmid">32524445</pub-id></element-citation></ref><ref id="B18-healthcare-09-01099"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Simonyan</surname><given-names>K.</given-names></name>
<name><surname>Zisserman</surname><given-names>A.</given-names></name>
</person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1409.1556</pub-id></element-citation></ref><ref id="B19-healthcare-09-01099"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
<name><surname>Ioffe</surname><given-names>S.</given-names></name>
<name><surname>Vanhoucke</surname><given-names>V.</given-names></name>
<name><surname>Alemi</surname><given-names>A.</given-names></name>
</person-group><article-title>Inception-v4, inception-resnet and the impact of residual connections on learning</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>San Francisco, CA, USA</conf-loc><conf-date>4&#x02013;9 February 2017</conf-date><volume>Volume 31</volume></element-citation></ref><ref id="B20-healthcare-09-01099"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Akiba</surname><given-names>T.</given-names></name>
<name><surname>Suzuki</surname><given-names>S.</given-names></name>
<name><surname>Fukuda</surname><given-names>K.</given-names></name>
</person-group><article-title>Extremely large minibatch sgd: Training resnet-50 on imagenet in 15 minutes</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1711.04325</pub-id></element-citation></ref><ref id="B21-healthcare-09-01099"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sandler</surname><given-names>M.</given-names></name>
<name><surname>Howard</surname><given-names>A.</given-names></name>
<name><surname>Zhu</surname><given-names>M.</given-names></name>
<name><surname>Zhmoginov</surname><given-names>A.</given-names></name>
<name><surname>Chen</surname><given-names>L.</given-names></name>
</person-group><article-title>MobileNetV2</article-title><source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;22 June 2018</conf-date><fpage>4510</fpage><lpage>4520</lpage></element-citation></ref><ref id="B22-healthcare-09-01099"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B23-healthcare-09-01099"><label>23.</label><element-citation publication-type="webpage"><article-title>Chest X-ray Images (Pneumonia)</article-title><comment>Available online: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia" ext-link-type="uri">https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-12-16">(accessed on 16 December 2020)</date-in-citation></element-citation></ref><ref id="B24-healthcare-09-01099"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Chollet</surname><given-names>F.</given-names></name>
</person-group><source>Deep Learning with Python</source><publisher-name>Simon and Schuster</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2017</year></element-citation></ref><ref id="B25-healthcare-09-01099"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>L.N.</given-names></name>
</person-group><article-title>A disciplined approach to neural network hyper-parameters: Part 1&#x02013;learning rate, batch size, momentum, and weight decay</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1803.09820</pub-id></element-citation></ref><ref id="B26-healthcare-09-01099"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>S.L.</given-names></name>
<name><surname>Kindermans</surname><given-names>P.J.</given-names></name>
<name><surname>Ying</surname><given-names>C.</given-names></name>
<name><surname>Le</surname><given-names>Q.V.</given-names></name>
</person-group><article-title>Don&#x02019;t decay the learning rate, increase the batch size</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1711.00489</pub-id></element-citation></ref><ref id="B27-healthcare-09-01099"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bergstra</surname><given-names>J.</given-names></name>
<name><surname>Bengio</surname><given-names>Y.</given-names></name>
</person-group><article-title>Random search for hyper-parameter optimization</article-title><source>J. Mach. Learn. Res.</source><year>2012</year><volume>13</volume><fpage>281</fpage><lpage>305</lpage></element-citation></ref><ref id="B28-healthcare-09-01099"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Perez</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
</person-group><article-title>The effectiveness of data augmentation in image classification using deep learning</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1712.04621</pub-id></element-citation></ref><ref id="B29-healthcare-09-01099"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Filipczuk</surname><given-names>P.</given-names></name>
<name><surname>Fevens</surname><given-names>T.</given-names></name>
<name><surname>Krzy&#x0017c;ak</surname><given-names>A.</given-names></name>
<name><surname>Monczak</surname><given-names>R.</given-names></name>
</person-group><article-title>Computer-aided breast cancer diagnosis based on the analysis of cytological images of fine needle biopsies</article-title><source>IEEE Trans. Med. Imaging</source><year>2013</year><volume>32</volume><fpage>2169</fpage><lpage>2178</lpage><pub-id pub-id-type="doi">10.1109/TMI.2013.2275151</pub-id><pub-id pub-id-type="pmid">23912498</pub-id></element-citation></ref><ref id="B30-healthcare-09-01099"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ahsan</surname><given-names>M.M.</given-names></name>
</person-group><source>Real Time Face Recognition in Unconstrained Environment</source><publisher-name>Lamar University</publisher-name><publisher-loc>Beaumont, TX, USA</publisher-loc><year>2018</year></element-citation></ref><ref id="B31-healthcare-09-01099"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Molnar</surname><given-names>C.</given-names></name>
</person-group><source>Interpretable Machine Learning</source><publisher-name>Leanpub</publisher-name><publisher-loc>Victoria, BC, Canada</publisher-loc><year>2020</year></element-citation></ref><ref id="B32-healthcare-09-01099"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Moore</surname><given-names>A.P.</given-names></name>
<name><surname>Prince</surname><given-names>S.J.</given-names></name>
<name><surname>Warrell</surname><given-names>J.</given-names></name>
<name><surname>Mohammed</surname><given-names>U.</given-names></name>
<name><surname>Jones</surname><given-names>G.</given-names></name>
</person-group><article-title>Superpixel lattices</article-title><source>Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>23&#x02013;28 June 2008</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B33-healthcare-09-01099"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khan</surname><given-names>A.I.</given-names></name>
<name><surname>Shah</surname><given-names>J.L.</given-names></name>
<name><surname>Bhat</surname><given-names>M.M.</given-names></name>
</person-group><article-title>Coronet: A Deep Neural Network for Detection and Diagnosis of COVID-19 from Chest X-ray Images</article-title><source>Comput. Methods Programs Biomed.</source><year>2020</year><volume>196</volume><fpage>105581</fpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105581</pub-id><pub-id pub-id-type="pmid">32534344</pub-id></element-citation></ref><ref id="B34-healthcare-09-01099"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Qin</surname><given-names>L.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Yin</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Kong</surname><given-names>B.</given-names></name>
<name><surname>Bai</surname><given-names>J.</given-names></name>
<name><surname>Lu</surname><given-names>Y.</given-names></name>
<name><surname>Fang</surname><given-names>Z.</given-names></name>
<name><surname>Song</surname><given-names>Q.</given-names></name>
<etal/>
</person-group><article-title>Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT</article-title><source>Radiology</source><year>2020</year><pub-id pub-id-type="doi">10.1148/radiol.2020200905</pub-id><pub-id pub-id-type="pmid">32191588</pub-id></element-citation></ref><ref id="B35-healthcare-09-01099"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Kang</surname><given-names>B.</given-names></name>
<name><surname>Ma</surname><given-names>J.</given-names></name>
<name><surname>Zeng</surname><given-names>X.</given-names></name>
<name><surname>Xiao</surname><given-names>M.</given-names></name>
<name><surname>Guo</surname><given-names>J.</given-names></name>
<name><surname>Cai</surname><given-names>M.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Meng</surname><given-names>X.</given-names></name>
<etal/>
</person-group><article-title>A deep learning algorithm using CT images to screen for Corona Virus Disease (COVID-19)</article-title><source>Eur. Radiol.</source><year>2021</year><volume>31</volume><fpage>6096</fpage><lpage>6104</lpage><pub-id pub-id-type="doi">10.1007/s00330-021-07715-1</pub-id><pub-id pub-id-type="pmid">33629156</pub-id></element-citation></ref><ref id="B36-healthcare-09-01099"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jin</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Deng</surname><given-names>L.</given-names></name>
<name><surname>Zheng</surname><given-names>C.</given-names></name>
<name><surname>Zhou</surname><given-names>J.</given-names></name>
<name><surname>Shi</surname><given-names>H.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
</person-group><article-title>Development and Evaluation of an AI System for COVID-19 Diagnosis</article-title><source>MedRxiv</source><year>2020</year><pub-id pub-id-type="doi">10.1101/2020.03.20.20039834</pub-id></element-citation></ref><ref id="B37-healthcare-09-01099"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Xie</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Shen</surname><given-names>C.</given-names></name>
<name><surname>Xia</surname><given-names>Y.</given-names></name>
</person-group><article-title>Covid-19 Screening on Chest X-ray Images Using Deep Learning Based Anomaly Detection</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.12338</pub-id></element-citation></ref><ref id="B38-healthcare-09-01099"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>Y.</given-names></name>
<name><surname>Zheng</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Huang</surname><given-names>Z.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Jie</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>R.</given-names></name>
<etal/>
</person-group><article-title>Deep Learning Enables Accurate Diagnosis of Novel Coronavirus (COVID-19) with CT Images</article-title><source>MedRxiv</source><year>2020</year><pub-id pub-id-type="doi">10.1109/TCBB.2021.3065361</pub-id></element-citation></ref><ref id="B39-healthcare-09-01099"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Orujov</surname><given-names>F.</given-names></name>
<name><surname>Maskeli&#x0016b;nas</surname><given-names>R.</given-names></name>
<name><surname>Dama&#x00161;evi&#x0010d;ius</surname><given-names>R.</given-names></name>
<name><surname>Wei</surname><given-names>W.</given-names></name>
</person-group><article-title>Fuzzy based image edge detection algorithm for blood vessel detection in retinal images</article-title><source>Appl. Soft Comput.</source><year>2020</year><volume>94</volume><fpage>106452</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2020.106452</pub-id></element-citation></ref><ref id="B40-healthcare-09-01099"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Versaci</surname><given-names>M.</given-names></name>
<name><surname>Morabito</surname><given-names>F.C.</given-names></name>
</person-group><article-title>Image edge detection: A new approach based on fuzzy entropy and fuzzy divergence</article-title><source>Int. J. Fuzzy Syst.</source><year>2021</year><volume>23</volume><fpage>918</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1007/s40815-020-01030-5</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="healthcare-09-01099-f001"><label>Figure 1</label><caption><p>Comparison between a chest X-ray image analyzed by a doctor and a modified VGG16 model, wherein its layer &#x0201c;Block_4&#x0201d; drew particular attention to the collarbone and upper shoulder [<xref rid="B14-healthcare-09-01099" ref-type="bibr">14</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g001" position="float"/></fig><fig position="float" id="healthcare-09-01099-f002"><label>Figure 2</label><caption><p>The extraction of unnecessary or irrelevant features was reduced significantly following the analysis of a larger dataset [<xref rid="B14-healthcare-09-01099" ref-type="bibr">14</xref>].</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g002" position="float"/></fig><fig position="float" id="healthcare-09-01099-f003"><label>Figure 3</label><caption><p>Representative sample images of chest X-rays and CT scans used in the mixed dataset adopted for analysis.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g003" position="float"/></fig><fig position="float" id="healthcare-09-01099-f004"><label>Figure 4</label><caption><p>Confusion matrices of all models applied to the mixed test dataset.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g004" position="float"/></fig><fig position="float" id="healthcare-09-01099-f005"><label>Figure 5</label><caption><p>Plots of model accuracy and loss following each epoch applied to both training and testing datasets; TL = training loss; VL = validation loss; TA = training accuracy; VA = validation accuracy.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g005" position="float"/></fig><fig position="float" id="healthcare-09-01099-f006"><label>Figure 6</label><caption><p>AUC-ROC curves for all models using the test set.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g006" position="float"/></fig><fig position="float" id="healthcare-09-01099-f007"><label>Figure 7</label><caption><p>Representation of superpixels on sample images of chest X-rays and CT scans.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g007" position="float"/></fig><fig position="float" id="healthcare-09-01099-f008"><label>Figure 8</label><caption><p>Example of the varying number of features as the number of perturbation changes.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g008" position="float"/></fig><fig position="float" id="healthcare-09-01099-f009"><label>Figure 9</label><caption><p>Top four features that enabled the identification of COVID-19 patients from CT-scan-only and mixed datasets.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="healthcare-09-01099-g009" position="float"/></fig><table-wrap position="float" id="healthcare-09-01099-t001"><object-id pub-id-type="pii">healthcare-09-01099-t001_Table 1</object-id><label>Table 1</label><caption><p>Summary of the mixed dataset used in the analysis, including training and test sets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Label</th><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Train</th><th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Test</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chest X-ray</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CT scan</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chest X-ray</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CT scan</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Mixed Data</td><td align="center" valign="middle" rowspan="1" colspan="1">COVID-19</td><td align="center" valign="middle" rowspan="1" colspan="1">486</td><td align="center" valign="middle" rowspan="1" colspan="1">160</td><td align="center" valign="middle" rowspan="1" colspan="1">646</td><td align="center" valign="middle" rowspan="1" colspan="1">122</td><td align="center" valign="middle" rowspan="1" colspan="1">40</td><td align="center" valign="middle" rowspan="1" colspan="1">162</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Non-COVID-19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1266</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1426</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">317</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">357</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1752</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">320</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2072</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">439</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">519</td></tr></tbody></table></table-wrap><table-wrap position="float" id="healthcare-09-01099-t002"><object-id pub-id-type="pii">healthcare-09-01099-t002_Table 2</object-id><label>Table 2</label><caption><p>Superpixel calculation parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Function</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Kernel size</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Maximum distance</td><td align="center" valign="middle" rowspan="1" colspan="1">200</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ratio</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="healthcare-09-01099-t003"><object-id pub-id-type="pii">healthcare-09-01099-t003_Table 3</object-id><label>Table 3</label><caption><p>COVID-19 screening performance of all models using a mixed dataset, presented with 95% confidence intervals (CI, <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm10" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>&#x02014;Training Set; <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm11" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>&#x02014;Test Set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Algorithm</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Accuracy (%)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Precision (%)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Recall (%)</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">F-1 Score (%)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CI</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CI</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CI</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">T<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow/><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CI</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" rowspan="1" colspan="1">95</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">93 &#x000b1; 1.4</td><td align="center" valign="middle" rowspan="1" colspan="1">95</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">94 &#x000b1; 1.3</td><td align="center" valign="middle" rowspan="1" colspan="1">95</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">93 &#x000b1; 1.4</td><td align="center" valign="middle" rowspan="1" colspan="1">95</td><td align="center" valign="middle" rowspan="1" colspan="1">92</td><td align="center" valign="middle" rowspan="1" colspan="1">93.5 &#x000b1; 1.34</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">InceptionResNetV2</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">93.5 &#x000b1; 1.34</td><td align="center" valign="middle" rowspan="1" colspan="1">95</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">94 &#x000b1; 1.3</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">93.5 &#x000b1; 1.34</td><td align="center" valign="middle" rowspan="1" colspan="1">94</td><td align="center" valign="middle" rowspan="1" colspan="1">93</td><td align="center" valign="middle" rowspan="1" colspan="1">93.5 &#x000b1; 1.35</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">85</td><td align="center" valign="middle" rowspan="1" colspan="1">86.5 &#x000b1; 1.86</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">85</td><td align="center" valign="middle" rowspan="1" colspan="1">86 &#x000b1; 1.89</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">85</td><td align="center" valign="middle" rowspan="1" colspan="1">86.5 &#x000b1; 1.86</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">85</td><td align="center" valign="middle" rowspan="1" colspan="1">86 &#x000b1; 1.89</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MobileNetV2</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">95 &#x000b1; 1.2</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td><td align="center" valign="middle" rowspan="1" colspan="1">92</td><td align="center" valign="middle" rowspan="1" colspan="1">95.5 &#x000b1; 1.13</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">95 &#x000b1; 1.2</td><td align="center" valign="middle" rowspan="1" colspan="1">99</td><td align="center" valign="middle" rowspan="1" colspan="1">91</td><td align="center" valign="middle" rowspan="1" colspan="1">95 &#x000b1; 1.2</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet101</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">86</td><td align="center" valign="middle" rowspan="1" colspan="1">87 &#x000b1; 1.83</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">87</td><td align="center" valign="middle" rowspan="1" colspan="1">87.5 &#x000b1; 1.80</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">86</td><td align="center" valign="middle" rowspan="1" colspan="1">87 &#x000b1; 1.83</td><td align="center" valign="middle" rowspan="1" colspan="1">88</td><td align="center" valign="middle" rowspan="1" colspan="1">86</td><td align="center" valign="middle" rowspan="1" colspan="1">87 &#x000b1; 1.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">VGG19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.5 &#x000b1; 1.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93 &#x000b1; 1.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.5 &#x000b1; 1.43</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93 &#x000b1; 1.4</td></tr></tbody></table></table-wrap><table-wrap position="float" id="healthcare-09-01099-t004"><object-id pub-id-type="pii">healthcare-09-01099-t004_Table 4</object-id><label>Table 4</label><caption><p>Top-performing models in terms of accuracy and different datasets adopted.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Datasize</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">X-ray</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">VGG16</td><td align="center" valign="middle" rowspan="1" colspan="1">98.5 &#x000b1; 1.191</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">MobileNetV2</td><td align="center" valign="middle" rowspan="1" colspan="1">98.5 &#x000b1; 1.191</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CT-Scan</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">MobileNetV2</td><td align="center" valign="middle" rowspan="1" colspan="1">94 &#x000b1; 2.327</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mixed-data</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2591</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MobileNetV2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95 &#x000b1; 1.12</td></tr></tbody></table></table-wrap><table-wrap position="float" id="healthcare-09-01099-t005"><object-id pub-id-type="pii">healthcare-09-01099-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison between previous studies found in the literature and our present study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reference</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset Size</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Li et al. (2020)&#x000a0;[<xref rid="B34-healthcare-09-01099" ref-type="bibr">34</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">4356</td><td align="center" valign="middle" rowspan="1" colspan="1">90%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Wang et al. (2021)&#x000a0;[<xref rid="B35-healthcare-09-01099" ref-type="bibr">35</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Inception-M</td><td align="center" valign="middle" rowspan="1" colspan="1">1065</td><td align="center" valign="middle" rowspan="1" colspan="1">74%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Zhang et al. (2020)&#x000a0;[<xref rid="B37-healthcare-09-01099" ref-type="bibr">37</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">1531</td><td align="center" valign="middle" rowspan="1" colspan="1">90%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Song et al. (2020)&#x000a0;[<xref rid="B38-healthcare-09-01099" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">274</td><td align="center" valign="middle" rowspan="1" colspan="1">86%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Chen et al. (2020)&#x000a0;[<xref rid="B6-healthcare-09-01099" ref-type="bibr">6</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">UNet<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="mm20" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow/><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle" rowspan="1" colspan="1">133</td><td align="center" valign="middle" rowspan="1" colspan="1">98.5%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Jin et al. (2020)&#x000a0;[<xref rid="B36-healthcare-09-01099" ref-type="bibr">36</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1882</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.1%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This study</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MobileNetV2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.5% &#x000b1; 1.19%</td></tr></tbody></table></table-wrap></floats-group></article>