Plan Overview

    Modify the Model: Add a bounding box prediction head to the model.
    Extend the Dataset Class: Update the dataset class to include bounding box data.
    Update the Loss Function: Include a bounding box loss in the combined loss function.
    Update the Training Loop: Ensure the training loop uses the new loss function and handles bounding box predictions.

Step-by-Step Implementation
1. Modify the Model

Add a bounding box prediction head to the LlavaMptForCausalLM class in train.py:

python

class LlavaMptForCausalLM(LlavaMpt):
    def __init__(self, config):
        super().__init__(config)
        self.bbox_head = torch.nn.Linear(self.hidden_size, 4)  # Assuming 4 coordinates for bbox

    def forward(self, input_ids=None, bbox=None, **kwargs):
        outputs = super().forward(input_ids, **kwargs)
        bbox_pred = self.bbox_head(outputs.last_hidden_state[:, 0, :])  # Assuming [CLS] token for bbox pred
        return outputs, bbox_pred

2. Extend the Dataset Class

Update the LazySupervisedDataset class in train.py to include bounding box data:

python

class LazySupervisedDataset(Dataset):
    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, data_args: DataArguments):
        super(LazySupervisedDataset, self).__init__()
        list_data_dict = json.load(open(data_path, "r"))
        rank0_print("Formatting inputs...Skip in lazy mode")
        self.tokenizer = tokenizer
        self.list_data_dict = list_data_dict
        self.data_args = data_args

    def __len__(self):
        return len(self.list_data_dict)

    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        sources = self.list_data_dict[i]
        if isinstance(i, int):
            sources = [sources]
        assert len(sources) == 1

        if 'image' in sources[0]:
            image_file = self.list_data_dict[i]['image']
            image_folder = self.data_args.image_folder
            processor = self.data_args.image_processor
            image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')
            if self.data_args.image_aspect_ratio == 'pad':
                def expand2square(pil_img, background_color):
                    width, height = pil_img.size
                    if width == height:
                        return pil_img
                    elif width > height:
                        result = Image.new(pil_img.mode, (width, width), background_color)
                        result.paste(pil_img, (0, (width - height) // 2))
                        return result
                    else:
                        result = Image.new(pil_img.mode, (height, height), background_color)
                        result.paste(pil_img, ((height - width) // 2, 0))
                        return result
                image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))
                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
            else:
                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
            sources = preprocess_multimodal(copy.deepcopy([e["conversations"] for e in sources]), self.data_args)
        else:
            sources = copy.deepcopy([e["conversations"] for e in sources])
        data_dict = preprocess(sources, self.tokenizer, has_image=('image' in self.list_data_dict[i]))
        if isinstance(i, int):
            data_dict = dict(input_ids=data_dict["input_ids"][0], labels=data_dict["labels"][0])

        if 'image' in self.list_data_dict[i]:
            data_dict['image'] = image
            if 'bbox' in self.list_data_dict[i]:
                data_dict['bbox'] = torch.tensor(self.list_data_dict[i]['bbox'])
            else:
                data_dict['bbox'] = torch.zeros(4)
        elif self.data_args.is_multimodal:
            crop_size = self.data_args.image_processor.crop_size
            data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])
            data_dict['bbox'] = torch.zeros(4)

        return data_dict

3. Update the Loss Function

Include a bounding box loss in the combined loss function in train.py:

python

def compute_loss(model, inputs, return_outputs=False):
    labels = inputs.pop("labels")
    bbox_targets = inputs.pop("bbox")
    outputs, bbox_pred = model(**inputs)

    loss_fct = torch.nn.CrossEntropyLoss()
    loss = loss_fct(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))

    bbox_loss_fct = torch.nn.SmoothL1Loss()
    bbox_loss = bbox_loss_fct(bbox_pred, bbox_targets)

    total_loss = loss + bbox_loss
    return (total_loss, outputs) if return_outputs else total_loss





UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED 
UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED 
UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED UNFINSIHED  UNFINSIHED 



4. Update the Training Loop

Ensure the training loop uses the new loss function and handles bounding box predictions:

python

def train(attn_implementation=None):
    global local_rank

    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    local_rank = training_args.local_rank
    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))

    bnb_model_from_pretrained_args = {}
    if training_args.bits in [4, 8]:
        from transformers import BitsAndBytesConfig
        bnb_model_from_pretrained_args.update(dict(
            device_map={"": training_args.device},
            load_in_4bit=training_args.bits == 4,
            load_in_8bit=training_args.bits == 8,
            quantization_config=BitsAndBytesConfig(
                load_in_4bit=training_args.bits == 4,
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=training_args.double_quant,
                bnb_4bit_quant_type=training_args.quant_type
            )
        ))

    if model_args.vision_tower is not None:
        if 'mpt' in model_args.model_name_or_path:
            config = transformers.AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)
            config.attn_config['attn_impl'] = training_args.mpt_attn_impl
            model = LlavaMptForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                config=config,
                cache_dir=training_args.cache_dir,
                **bnb_model_from_pretrained_args
            )
        else:
            from llava.model.builder import load_pretrained_model
            tokenizer, model, image_processor, context_len = load_pretrained_model(
                model_path='/users/jjls2000/sharedscratch/Dissertation/checkpoints/llava-med-v1.5-mistral-7b',
                model_base=None,
                model_name='llava-med-v1.5-mistral-7b'
            )
    else:
        model = transformers.LlamaForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            cache_dir=training_args.cache_dir,
            attn_implementation=attn_implementation,
            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),
            **bnb_model_from_pretrained_args
        )
    model.config.use_cache = False

    if model_args.freeze_backbone:
        model.model.requires_grad_(False)

    if training_args.bits in [4, 8]:
        from peft import prepare_model_for_kbit_training
        model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)

    if training_args.gradient_checkpointing:
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
        else:
            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)
            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

    if training_args.lora_enable:
        from peft import LoraConfig, get_peft_model
        lora_config = LoraConfig(
            r=training_args.lora_r,
            lora_alpha=training_args.lora_alpha,
            target_modules=find_all_linear_names(model),
            lora_dropout=training_args.lora_dropout,
            bias=training_args.lora_bias,
            task_type="CAUSAL_LM",
        )
        if training_args.bits == 16:
            if training_args.bf16:
                model.to(torch.bfloat16)
            if training_args.fp16:
                model.to(torch.float16)
        rank0_print("Adding LoRA adapters...")
        model = get_peft_model(model, lora_config)

    if 'mpt' in model_args.model_name_or_path:
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_args.model_name_or_path,
            cache_dir=training_args.cache_dir,
            model_max_length=training_args.model_max_length,
            padding_side="right"
        )
